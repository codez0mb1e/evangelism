{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello Spark Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local Spark Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "  session = SparkSession.builder.appName(\"HelloSparkApp\").getOrCreate()\n",
    "  cntx = session.sparkContext\n",
    "\n",
    "  return session,cntx\n",
    "\n",
    "session,cntx = init_spark()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = cntx.parallelize(range(0, 11))\n",
    "\n",
    "print(nums.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a RDD containing lines from [AI-Workshop](https://github.com/codez0mb1e/evangelism/tree/master/AI-Workshop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('../../AI-Workshop/README.md')\n",
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPartitions = lines.getNumPartitions() \n",
    "print('Number of partitions storing the dataset: {}'.format(numPartitions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words and assign a frequency of 1 to each word\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Filter stop words\n",
    "stop_words = ['', '*', '##', 'и', 'с', 'в','по']\n",
    "filtered_words = words.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Get word tuples\n",
    "word_tuples = filtered_words.map(lambda word: (word, 1))\n",
    "\n",
    "# Count the frequency for words\n",
    "counts = word_tuples.reduceByKey(operator.add) # equals .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort the counts in descending order based on the word frequency\n",
    "sorted_counts = counts.sortBy(lambda x: x[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an iterator over the counts to print a word and its frequency\n",
    "for word,count in sorted_counts.toLocalIterator():\n",
    "    print('{} --> {}'.format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store resut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pyspark sql Row class\n",
    "from pyspark.sql import Row \n",
    "\n",
    "# create a table from Rows\n",
    "word_counts_rows = sorted_counts.map(lambda p: Row(word=p[0], count=int(p[1])))\n",
    "word_counts_table = sqlContext.createDataFrame(word_counts_rows) \n",
    "\n",
    "# register a temp table for querying\n",
    "word_counts_table.registerTempTable(\"word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data fron `word_counts_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dt = session.sql('SELECT word, count FROM word_count WHERE length(word) > 2 and count > 1')\n",
    "word_counts_dt.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read word statistics from Spark table and draw **word cloud**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stats = word_counts_dt.toPandas()\n",
    "words_stats = words_stats.set_index('word').to_dict()['count']\n",
    "\n",
    "# Fit word cloud\n",
    "wordcloud = WordCloud().fit_words(words_stats)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 - Spark (local)",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
